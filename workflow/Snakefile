# The main entry point of your workflow.
# After configuring, running snakemake -n in a clone of this repository should successfully execute a dry-run of the workflow.
import itertools
import os 
import glob
import sys
import pandas as pd

configfile: "../config/config.yaml"
include: "rules/common.smk"
include: "rules/metagenomics.smk"
localrules: multiqc_trimmed

rule all:
    input:
        expand("../results/{dataset}/filtered/{sample}.filtered.R1.fastq.gz",
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  FastP R2
        expand("../results/{dataset}/filtered/{sample}.filtered.R2.fastq.gz", 
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  FastP R2
        expand("../results/{dataset}/filtered/fastqc/{sample}.filtered.R1_fastqc.html",
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  FastQC
        expand("../results/{dataset}/bwa/{sample}.mapped.bam",
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  BWA
        expand("../results/{dataset}/bwa/{sample}.unmapped.bam",
            zip, sample=samples["sample"], dataset=samples["dataset"]),  # BWA Unmapped
        expand("../results/{dataset}/bwa/{sample}.clean.R1.fastq",
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  BWA fastq R1
        expand("../results/{dataset}/bwa/{sample}.clean.R2.fastq",
            zip, sample=samples["sample"], dataset=samples["dataset"]), #  BWA fastq R2
        directory("../resources/metaphlan_db"), #  Metaphlan setup/Database
        expand("../results/{dataset}/abundance/metaphlan/{sample}.metaphlan_profile.txt",
            zip, sample=samples["sample"], dataset=samples["dataset"]), # Metaphlan profiles
        "../results/allDatasets/metaphlan/merged_abundance_table.allDatasets.txt", #  Metaphlan merged abundance table
        "../results/allDatasets/metaphlan/merged_abundance_table.species.allDatasets.txt", #  Metaphlan species abundance table
        "../results/allDatasets/metaphlan/merged_abundance_table.genus.allDatasets.txt", #  " " genus
        "../results/allDatasets/metaphlan/abundance_heatmap_species.allDatasets.png",
        "../resources/kaiju_head/kaiju-v1.8.0-linux-x86_64.tar.gz", # Kaiju Tar
        "../resources/kaiju_head/kaijuDB/kaiju_db_refseq_2021-02-26.tgz", # Kaiju DB download
        expand("../results/{dataset}/abundance/kaiju_refseq/{sample}.kaiju_refseq.txt",
            zip, sample=samples["sample"], dataset=samples["dataset"]), # Kaiju refseq outputs
        "../results/allDatasets/kaiju/kaiju_abundance_table.species.allDatasets.txt", # Kaiju refseq summary

        directory("../resources/kneaddata"), # KneadData DB install
        expand("../results/{dataset}/kneaddata/{sample}/{sample}_R1_001_kneaddata_paired_1.fastq",
            zip, sample=samples["sample"], dataset=samples["dataset"]), # KneadData R2
        expand("../results/{dataset}/kneaddata/{sample}/{sample}_R1_001_kneaddata_paired_2.fastq",
            zip, sample=samples["sample"], dataset=samples["dataset"]) # KneadData R1

         #hclust2.py -i merged_abundance_table.species.allDatasets.txt -o abundance_heatmap.species.allDatasets.png --f_dist_f braycurtis --s_dist_f braycurtis --cell_aspect_ratio 0.5 -l --flabel_size 6 --slabel_size 10 --max_flabel_len 100 --max_slabel_len 100 --minv 0.1 --dpi 300
