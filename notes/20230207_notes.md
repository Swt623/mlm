# SCRIPT OG Metagenomics Data

# 20230207

concatenated reads from Batch_04A1 and Batch_04A2

Starting analysis with updated processing

Making sample sheet
    python3 workflow/scripts/prep_sample_sheet.py --delimiter="_S" --path="/scratch/jsj3921/Batch_01" --subdirectories=True
    python3 workflow/scripts/prep_sample_sheet.py --delimiter="_S" --path="/scratch/jsj3921/Batch_02" --subdirectories=True --out="./config/sample_sheet_02.tsv"
    python3 workflow/scripts/prep_sample_sheet.py --delimiter="_S" --path="/scratch/jsj3921/Batch_03" --subdirectories=True --out="./config/sample_sheet_03.tsv"
    python3 workflow/scripts/prep_sample_sheet.py --delimiter="_R" --r1_common="R1" --r2_common="R2" --path="/scratch/jsj3921/Batch_04A12" --subdirectories=True --out="./config/sample_sheet_04.tsv"
    python3 workflow/scripts/prep_sample_sheet.py --delimiter="_R" --r1_common="R1" --r2_common="R2" --path="/scratch/jsj3921/Batch_04B" --subdirectories=True --out="./config/sample_sheet_04b.tsv"

Added 202221-Communal-Zymo to end of sample sheet as bioinformatic control

snakemake worked great

the following are 100% unknown from metaphlan 
     [1] "DNA_B03_21"  "DNA_B04_101" "DNA_B04_106" "DNA_B04_139" "DNA_B04_141" "DNA_B04_151" "DNA_B04_48"  "DNA_B04_61"  "DNA_B04_95"  "DNA_B04_98" 


# 20230109

Working on getting humann started
    Installing metaphlan 4 database required excessive RAM so had to do it in a batch job
    
    mamba activate /projects/b1180/software/conda_envs/humann/

    metaphlan 20221221-Comunal-Zymo.fastp_bowtie.r1.fastq.gz,20221221-Comunal-Zymo.fastp_bowtie.r2.fastq.gz --nproc 23 --input_type fastq -o mpatest.tsvls

Recent results from kraken2 and metaphlan output suggests a really interesting correlational relationship between groups of taxa 
This could be that there are taxa that have low competition with each other and thus co-associate and strong competition with others
that leads them to be co-exclusive
Some other taxa are just ok with being there independent of the other cohorts. Id hypothesize that these taxa provide some metabolic 
niche necessary for others and/or have different ecological constraints compared to the cohorts. That is, they can metabolize some 
rare but lung-abundant nutrient source. 
    Would be interesting if functional abundance profiles mimic the above, then find specific predictive metabolites

testing merging on humann/metaphlan output

module load BBMap
Usage for paired files:         bbmerge.sh in1=<read1> in2=<read2> out=<merged reads> outu1=<unmerged1> outu2=<unmerged2>
bbmerge.sh in1=20221221-Comunal-Zymo.fastp_bowtie.r1.fastq.gz in2=20221221-Comunal-Zymo.fastp_bowtie.r2.fastq.gz out=merged_test.fastq.gz outu1=unmergedout.1.fastq.gz outu2=unmergedout.2.fastq.gz

bbmerge.sh in1=20221221-Comunal-Zymo.fastp_bowtie.r1.fastq.gz \
    in2=20221221-Comunal-Zymo.fastp_bowtie.r2.fastq.gz \
    out=merged_test.fastq.gz \
    outu1=unmergedout.1.fastq.gz \
    outu2=unmergedout.2.fastq.gz \
    verystrict=t \
    k=60 \
    ihist=ihist.txt \
    mininsert=90
(will automatically make fast.gz)
- very strict = good for assembly based approached/reduced FP 
- k=60 is the kmer length used for overlap analysis/graph. Default is 31 but 60 is good for 150 PE
- mininsert=90 means that the min merged read length will be 90, which is noted in the literature as an optimal minimum for functional profiling

humann --input merged_test.fastq.gz --output humann_merged --threads 40 --memory-use maximum





from snakemake.utils import validate
import pandas as pd

### Helper functions for getting initial reads ###

def get_read_path_v2(wildcards):
    return samples.loc[wildcards.sample, ["sample", "dataset", "forward_read", "reverse_read"]].dropna()

def get_r1(wildcards):
    tmp = get_read_path_v2(wildcards)
    return tmp["forward_read"]

def get_r2(wildcards):
    tmp = get_read_path_v2(wildcards)
    return tmp["reverse_read"]


def get_rules(wildcards):
    all_rules = []
    if config["FASTQC"]:
        if config["TRIM_READS"]:
            all_rules = all_rules = all_rules + expand(
                "results/fastqc_out/fastp_qc/{sample}/{sample}.fastp.r1_fastqc.html", 
                sample=samples["sample"]
            )
            all_rules = all_rules = all_rules + expand(
                "results/fastqc_out/fastp_qc/{sample}/{sample}.fastp.r2_fastqc.html", 
                sample=samples["sample"]
            )
        if config["DECONVOLUTE"]:
            pass
            #all_rules = all_rules = all_rules + expand(
            #    "results/fastqc_out/bwa_qc/{sample}/{sample}.fastp_bwa.r1_fastqc.html", 
            #    sample=samples["sample"]
            #)
            #all_rules = all_rules = all_rules + expand(
            #    "results/fastqc_out/bwa_qc/{sample}/{sample}.fastp_bwa.r2_fastqc.html", 
            #    sample=samples["sample"]
            #)
    if config["TRIM_READS"]:
        all_rules = all_rules + expand(
            "results/fastp_out/{sample}/{sample}.fastp.r1.fastq.gz", 
            sample=samples["sample"])
        all_rules = all_rules + expand(
            "results/fastp_out/{sample}/{sample}.fastp.r2.fastq.gz", 
            sample=samples["sample"])
    if config["DECONVOLUTE"]:
        if config["BWA"]:
            all_rules = all_rules + expand(
                "results/bwa_out/{sample}/{sample}.fastp_bwa.r1.fastq", 
                sample=samples["sample"])
            all_rules = all_rules + expand(
                "results/bwa_out/{sample}/{sample}.fastp_bwa.r2.fastq", 
                sample=samples["sample"])
        if config["BOWTIE2"]:
            all_rules = all_rules + expand(
                "results/bowtie_out/{sample}/{sample}.fastp_bowtie.r1.fastq.gz", 
                sample=samples["sample"])
            all_rules = all_rules + expand(
                "results/bowtie_out/{sample}/{sample}.fastp_bowtie.r2.fastq.gz", 
                sample=samples["sample"]) 
            all_rules.append("results/bowtie_out/flagstat_summary.txt")
        if config["NONPAREIL"]:
            all_rules = all_rules + expand(
                "results/nonpareil_out/{sample}/{sample}.npo", 
                sample=samples["sample"])
    if config["METAPHLAN"]:
        all_rules.append("results/metaphlan_bowtie_out/merged_metaphlan_profile_genus.tsv")
        all_rules.append("results/metaphlan_bowtie_out/merged_metaphlan_profile_species.tsv")
    if config["KRAKEN2"]:
        all_rules = all_rules + expand("results/kraken/{sample}/{sample}_kraken2out.txt", sample=samples["sample"])
    if config["METAXA2"]:
        all_rules = all_rules + expand("results/metaxa2/{sample}/{sample}_metaxa2.taxonomy.txt", sample=samples["sample"])
    if config["ASSEMBLE"]:
        if config["MEGAHIT"]:
            all_rules = all_rules + expand(
                "results/megahit_out/{sample}/{sample}.contigs.fa", 
                sample=samples["sample"])
        if config["SPADES"]:
            all_rules = all_rules + expand(
                "results/spades_out/{sample}/scaffolds.fasta", 
                sample=samples["sample"])
        if config["SPADES"] or config["MEGAHIT"]:
            all_rules.append("results/quast_out/multiqc_report.html")
            all_rules = all_rules + expand(
                "results/{assembler}_parsed/{sample}/{sample}.parsed_assembly.fa", 
                sample=samples["sample"],
                assembler=ASSEMBLER)
    if config["METABAT2"]:
        metabat2_results = directory(expand(
            "results/metabat_{assembler}_out/{sample}/bins/", 
            assembler=ASSEMBLER, #["megahit", "spades"]
            sample=samples["sample"]
            Z))
        all_rules = all_rules + metabat2_results
    return all_rules

### Helper functions for configuring quast multiqc input ###

def get_multiqc_quast_input(wildcards):
    if config["MEGAHIT"] and config["SPADES"]:
        quast_reports = expand(
            "results/quast_out/{assembler}/{sample}/report.html", 
            sample=samples["sample"],
            assembler=ASSEMBLER #["megahit", "spades"])
        return quast_reports
    elif config["MEGAHIT"] ^ config["SPADES"]:
        if config["MEGAHIT"]:
            quast_reports=expand(
                "results/quast_out/megahit/{sample}/report.html", 
                sample=samples["sample"])
        elif config["SPADES"]:
            quast_reports=expand(
                "results/quast_out/spades/{sample}/report.html", 
                sample=samples["sample"])
        return quast_reports
    else:
        print("CHECK that at least one assembler is set to True in config.yaml")

### Parse config file to set select assembler options as wildcard list ###

def get_assemblers():
    if config["MEGAHIT"] and config["SPADES"]:
        ASSEMBLER = ["megahit", "spades"]
    elif config["MEGAHIT"] ^ config["SPADES"]:
            if config["MEGAHIT"]:
                ASSEMBLER = ["megahit"]
            elif config["SPADES"]:
                ASSEMBLER = ["megahit"]
    return ASSEMBLER

samples = pd.read_csv(config["samples"], sep="\t").set_index("sample", drop=False)
samples.index.names = ["sample"]

ASSEMBLER = get_assemblers()
